{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: KNN -- Doh"
      ],
      "metadata": {
        "id": "d--Ks9AXlVaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def knn_impute_by_user(matrix, valid_data, k):\n",
        "    \"\"\" Fill in the missing values using k-Nearest Neighbors based on\n",
        "    student similarity. Return the accuracy on valid_data.\n",
        "\n",
        "    See https://scikit-learn.org/stable/modules/generated/sklearn.\n",
        "    impute.KNNImputer.html for details.\n",
        "\n",
        "    :param matrix: 2D sparse matrix\n",
        "    :param valid_data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param k: int\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    nbrs = KNNImputer(n_neighbors=k)\n",
        "    # We use NaN-Euclidean distance measure.\n",
        "    mat = nbrs.fit_transform(matrix)\n",
        "    acc = sparse_matrix_evaluate(valid_data, mat)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def knn_impute_by_item(matrix, valid_data, k):\n",
        "    \"\"\" Fill in the missing values using k-Nearest Neighbors based on\n",
        "    question similarity. Return the accuracy on valid_data.\n",
        "\n",
        "    :param matrix: 2D sparse matrix\n",
        "    :param valid_data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param k: int\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Implement the function as described in the docstring.             #\n",
        "    #####################################################################\n",
        "    imputer = KNNImputer(n_neighbors=k)\n",
        "    mat = imputer.fit_transform(np.transpose(matrix))\n",
        "    acc = sparse_matrix_evaluate(valid_data, np.transpose(mat))\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return acc\n",
        "\n",
        "\n",
        "def main():\n",
        "    sparse_matrix = load_train_sparse(\"../data\").toarray()\n",
        "    val_data = load_valid_csv(\"../data\")\n",
        "    test_data = load_public_test_csv(\"../data\")\n",
        "\n",
        "    print(\"Sparse matrix:\")\n",
        "    print(sparse_matrix)\n",
        "    print(\"Shape of sparse matrix:\")\n",
        "    print(sparse_matrix.shape)\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Compute the validation accuracy for each k. Then pick k* with     #\n",
        "    # the best performance and report the test accuracy with the        #\n",
        "    # chosen k*.                                                        #\n",
        "    #####################################################################\n",
        "    list_k = [1, 6, 11, 16, 21, 26]\n",
        "    acc_user = []\n",
        "    acc_item = []\n",
        "\n",
        "    for k in list_k:\n",
        "        acc_user.append(knn_impute_by_user(sparse_matrix, val_data, k))\n",
        "        acc_item.append(knn_impute_by_item(sparse_matrix, val_data, k))\n",
        "\n",
        "    # Plot k vs. Validation Accuracy\n",
        "    p = plt.figure()\n",
        "    plt.plot(list_k, acc_user, label=\"Validation Accuracy (user)\")\n",
        "    plt.plot(list_k, acc_item, label=\"Validation Accuracy (item)\")\n",
        "    plt.title(\"k vs. Validation Accuracy\")\n",
        "    plt.xlabel(\"Value of k\")\n",
        "    plt.ylabel(\"Validation Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    p.savefig(\"final_knn.pdf\")\n",
        "\n",
        "    # Report Accuracies\n",
        "    for i in range(len(list_k)):\n",
        "        print(\"Validation Accuracy (user) for \", list_k[i], \" : \", acc_user[i])\n",
        "    for i in range(len(list_k)):\n",
        "        print(\"Validation Accuracy (item) for \", list_k[i], \" : \", acc_item[i])\n",
        "\n",
        "    # Find k* and report\n",
        "    max_k_user = list_k[acc_user.index(max(acc_user))]\n",
        "    max_k_item = list_k[acc_item.index(max(acc_item))]\n",
        "\n",
        "    print(\"k with Maximum Validation Accuracy (user): \", max_k_user)\n",
        "    print(\"k with Maximum Validation Accuracy (item): \", max_k_item)\n",
        "\n",
        "    # Report test accuracies\n",
        "    test_acc_user = knn_impute_by_user(sparse_matrix, test_data, max_k_user)\n",
        "    test_acc_item = knn_impute_by_item(sparse_matrix, test_data, max_k_item)\n",
        "    print(\"Test Accuracy (user) with k* = \", max_k_user, \" : \", test_acc_user)\n",
        "    print(\"Test Accuracy (item) with k* = \", max_k_item, \" : \", test_acc_item)\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "\n",
        "    # Report\n",
        "\n",
        "    # d) The test accuracy of user-based collaborative filtering with k* = 11\n",
        "    # was about 0.6842, and the test accuracy of item-based collaborative\n",
        "    # filtering with k* = 21 was about 0.6816.\n",
        "    # User-based collaborative filtering has slightly higher test accuracy by\n",
        "    # about 0.0026 difference.\n",
        "\n",
        "    # e) 1. It might return inaccurate predictions if there is no / a few\n",
        "    #       student(s) who answered the other question similarly, or simple\n",
        "    #       there is not enough amount of data (answers from other students).\n",
        "    #    2. The computational costs such as time and storage might be expensive\n",
        "    #       if the data is huge.\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "3E13GHsElac8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Information Response Theory -- Doh"
      ],
      "metadata": {
        "id": "ZpJiCnanlLwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from utils import *\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\" Apply sigmoid function.\n",
        "    \"\"\"\n",
        "    return np.exp(x) / (1 + np.exp(x))\n",
        "\n",
        "\n",
        "def neg_log_likelihood(data, theta, beta):\n",
        "    \"\"\" Compute the negative log-likelihood.\n",
        "\n",
        "    You may optionally replace the function arguments to receive a matrix.\n",
        "\n",
        "    :param data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param theta: Vector\n",
        "    :param beta: Vector\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Implement the function as described in the docstring.             #\n",
        "    #####################################################################\n",
        "    log_lklihood = 0\n",
        "\n",
        "    for index in range(len(data[\"user_id\"])):\n",
        "        if data[\"is_correct\"][index] == 1:\n",
        "            log_lklihood += \\\n",
        "                math.log(sigmoid(theta[data[\"user_id\"][index]][0] -\n",
        "                                 beta[data[\"question_id\"][index]][0]))\n",
        "        else:\n",
        "            log_lklihood += \\\n",
        "                math.log(1 - sigmoid(theta[data[\"user_id\"][index]][0] -\n",
        "                                     beta[data[\"question_id\"][index]][0]))\n",
        "\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return -log_lklihood\n",
        "\n",
        "\n",
        "def update_theta_beta(data, lr, theta, beta):\n",
        "    \"\"\" Update theta and beta using gradient descent.\n",
        "\n",
        "    You are using alternating gradient descent. Your update should look:\n",
        "    for i in iterations ...\n",
        "        theta <- new_theta\n",
        "        beta <- new_beta\n",
        "\n",
        "    You may optionally replace the function arguments to receive a matrix.\n",
        "\n",
        "    :param data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param lr: float\n",
        "    :param theta: Vector\n",
        "    :param beta: Vector\n",
        "    :return: tuple of vectors\n",
        "    \"\"\"\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Implement the function as described in the docstring.             #\n",
        "    #####################################################################\n",
        "    for index in range(len(data[\"user_id\"])):\n",
        "        if data[\"is_correct\"][index] == 1:\n",
        "            theta[data[\"user_id\"][index]][0] = \\\n",
        "                theta[data[\"user_id\"][index]][0] + \\\n",
        "                lr * sigmoid((-1) * theta[data[\"user_id\"][index]][0] +\n",
        "                             beta[data[\"question_id\"][index]][0])\n",
        "            beta[data[\"question_id\"][index]][0] = \\\n",
        "                beta[data[\"question_id\"][index]][0] - \\\n",
        "                lr * sigmoid((-1) * theta[data[\"user_id\"][index]][0] +\n",
        "                             beta[data[\"question_id\"][index]][0])\n",
        "        else:\n",
        "            theta[data[\"user_id\"][index]][0] = \\\n",
        "                theta[data[\"user_id\"][index]][0] - \\\n",
        "                lr * sigmoid(theta[data[\"user_id\"][index]][0] -\n",
        "                             beta[data[\"question_id\"][index]][0])\n",
        "            beta[data[\"question_id\"][index]][0] = \\\n",
        "                beta[data[\"question_id\"][index]][0] + \\\n",
        "                lr * sigmoid(theta[data[\"user_id\"][index]][0] -\n",
        "                             beta[data[\"question_id\"][index]][0])\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "    return theta, beta\n",
        "\n",
        "\n",
        "def irt(data, val_data, lr, iterations):\n",
        "    \"\"\" Train IRT model.\n",
        "\n",
        "    You may optionally replace the function arguments to receive a matrix.\n",
        "\n",
        "    :param data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param val_data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "    :param lr: float\n",
        "    :param iterations: int\n",
        "    :return: (theta, beta, val_acc_lst)\n",
        "    \"\"\"\n",
        "    # TODO: Initialize theta and beta.\n",
        "    theta = []\n",
        "    beta = []\n",
        "    for m in range(max(data[\"user_id\"]) + 1):\n",
        "        theta.append([0.1])\n",
        "    for n in range(max(data[\"question_id\"]) + 1):\n",
        "        beta.append([0.1])\n",
        "\n",
        "    theta = np.array(theta)\n",
        "    beta = np.array(beta)\n",
        "\n",
        "    val_acc_lst = []\n",
        "    neg_lld_list = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        neg_lld = neg_log_likelihood(data, theta=theta, beta=beta)\n",
        "        neg_lld_list.append(neg_lld)\n",
        "        score = evaluate(data=val_data, theta=theta, beta=beta)\n",
        "        val_acc_lst.append(score)\n",
        "        print(\"NLLK: {} \\t Score: {}\".format(neg_lld, score))\n",
        "        theta, beta = update_theta_beta(data, lr, theta, beta)\n",
        "\n",
        "    # TODO: You may change the return values to achieve what you want.\n",
        "    return theta, beta, val_acc_lst, neg_lld_list\n",
        "\n",
        "\n",
        "def evaluate(data, theta, beta):\n",
        "    \"\"\" Evaluate the model given data and return the accuracy.\n",
        "    :param data: A dictionary {user_id: list, question_id: list,\n",
        "    is_correct: list}\n",
        "\n",
        "    :param theta: Vector\n",
        "    :param beta: Vector\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    pred = []\n",
        "    for i, q in enumerate(data[\"question_id\"]):\n",
        "        u = data[\"user_id\"][i]\n",
        "        x = (theta[u] - beta[q]).sum()\n",
        "        p_a = sigmoid(x)\n",
        "        pred.append(p_a >= 0.5)\n",
        "    return np.sum((data[\"is_correct\"] == np.array(pred))) \\\n",
        "           / len(data[\"is_correct\"])\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_data = load_train_csv(\"../data\")\n",
        "    # You may optionally use the sparse matrix.\n",
        "    # sparse_matrix = load_train_sparse(\"../data\")\n",
        "    val_data = load_valid_csv(\"../data\")\n",
        "    test_data = load_public_test_csv(\"../data\")\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Tune learning rate and number of iterations. With the implemented #\n",
        "    # code, report the validation and test accuracy.                    #\n",
        "    #####################################################################\n",
        "    # Hyperparameters\n",
        "    lr = 0.1\n",
        "    iterations = 20\n",
        "\n",
        "    val_theta, val_beta, val_val_accu_list, val_neg_lld_list = \\\n",
        "        irt(train_data, val_data, lr, iterations)\n",
        "    print(\"Validation accuracies: \", val_val_accu_list)\n",
        "\n",
        "    iter_list = [i for i in range(0, iterations)]\n",
        "    p = plt.figure()\n",
        "    plt.plot(iter_list, val_neg_lld_list)\n",
        "    plt.title(\"Number of iteration vs. Negative log likelihood\")\n",
        "    plt.xlabel(\"Number of iteration\")\n",
        "    plt.ylabel(\"Negative log likelihood\")\n",
        "    # plt.show()\n",
        "    p.savefig(\"csc311_nllk.png\")\n",
        "\n",
        "    test_theta, test_beta, test_val_accu_list, test_neg_lld_list = \\\n",
        "        irt(train_data, test_data, lr, iterations)\n",
        "    print(\"Testing accuracies: \", test_val_accu_list)\n",
        "\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Implement part (d)                                                #\n",
        "    #####################################################################\n",
        "    j = [1, 10, 100]\n",
        "    j_beta = []\n",
        "    theta_i = [item for sublist in val_theta for item in sublist]\n",
        "    theta_i = sorted(theta_i)\n",
        "    prob_list = []\n",
        "\n",
        "    for n in j:\n",
        "        j_beta.append(val_beta[n][0])\n",
        "\n",
        "    for beta in j_beta:\n",
        "        prob = []\n",
        "        for theta in theta_i:\n",
        "            prob.append(sigmoid(theta - beta))\n",
        "        prob_list.append(prob)\n",
        "\n",
        "    p2 = plt.figure()\n",
        "    plt.plot(theta_i, prob_list[0], label=\"j_1 = 1\")\n",
        "    plt.plot(theta_i, prob_list[1], label=\"j_2 = 10\")\n",
        "    plt.plot(theta_i, prob_list[2], label=\"j_3 = 100\")\n",
        "    plt.title(\"Theta given j vs. Probability of correct answer\")\n",
        "    plt.xlabel(\"Theta given j\")\n",
        "    plt.ylabel(\"Probability of correct answer\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    p2.savefig(\"csc311_theta_prob.png\")\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6sTIw1OplbDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 3: Neural Networks -- Devansh\n",
        "\n"
      ],
      "metadata": {
        "id": "tOF4cXxNlDfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq2wF9DeIN_j",
        "outputId": "50e3ed88-fe7a-4257-b4d8-e15cd0d77448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hLW-MkuOlyIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import load_npz\n",
        "import csv\n",
        "import os"
      ],
      "metadata": {
        "id": "8BiykqzGl3t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##UTILS FUNCTIONS\n",
        "\n",
        "def _load_csv(path):\n",
        "    # A helper function to load the csv file.\n",
        "    if not os.path.exists(path):\n",
        "        raise Exception(\"The specified path {} does not exist.\".format(path))\n",
        "    # Initialize the data.\n",
        "    data = {\n",
        "        \"user_id\": [],\n",
        "        \"question_id\": [],\n",
        "        \"is_correct\": []\n",
        "    }\n",
        "    # Iterate over the row to fill in the data.\n",
        "    with open(path, \"r\") as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        for row in reader:\n",
        "            try:\n",
        "                data[\"question_id\"].append(int(row[0]))\n",
        "                data[\"user_id\"].append(int(row[1]))\n",
        "                data[\"is_correct\"].append(int(row[2]))\n",
        "            except ValueError:\n",
        "                # Pass first row.\n",
        "                pass\n",
        "            except IndexError:\n",
        "                # is_correct might not be available.\n",
        "                pass\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_train_sparse(root_dir=\"/data\"):\n",
        "    \"\"\" Load the training data as a spare matrix representation.\n",
        "\n",
        "    :param root_dir: str\n",
        "    :return: 2D sparse matrix\n",
        "    \"\"\"\n",
        "    path = os.path.join(root_dir, \"train_sparse.npz\")\n",
        "    if not os.path.exists(path):\n",
        "        raise Exception(\"The specified path {} \"\n",
        "                        \"does not exist.\".format(os.path.abspath(path)))\n",
        "    matrix = load_npz(path)\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def load_train_csv(root_dir=\"/data\"):\n",
        "    \"\"\" Load the training data as a dictionary.\n",
        "\n",
        "    :param root_dir: str\n",
        "    :return: A dictionary {user_id: list, question_id: list, is_correct: list}\n",
        "        WHERE\n",
        "        user_id: a list of user id.\n",
        "        question_id: a list of question id.\n",
        "        is_correct: a list of binary value indicating the correctness of\n",
        "        (user_id, question_id) pair.\n",
        "    \"\"\"\n",
        "    path = os.path.join(root_dir, \"train_data.csv\")\n",
        "    return _load_csv(path)\n",
        "\n",
        "\n",
        "def load_valid_csv(root_dir=\"/data\"):\n",
        "    \"\"\" Load the validation data as a dictionary.\n",
        "\n",
        "    :param root_dir: str\n",
        "    :return: A dictionary {user_id: list, question_id: list, is_correct: list}\n",
        "        WHERE\n",
        "        user_id: a list of user id.\n",
        "        question_id: a list of question id.\n",
        "        is_correct: a list of binary value indicating the correctness of\n",
        "        (user_id, question_id) pair.\n",
        "    \"\"\"\n",
        "    path = os.path.join(root_dir, \"valid_data.csv\")\n",
        "    return _load_csv(path)\n",
        "\n",
        "\n",
        "def load_public_test_csv(root_dir=\"/data\"):\n",
        "    \"\"\" Load the test data as a dictionary.\n",
        "\n",
        "    :param root_dir: str\n",
        "    :return: A dictionary {user_id: list, question_id: list, is_correct: list}\n",
        "        WHERE\n",
        "        user_id: a list of user id.\n",
        "        question_id: a list of question id.\n",
        "        is_correct: a list of binary value indicating the correctness of\n",
        "        (user_id, question_id) pair.\n",
        "    \"\"\"\n",
        "    path = os.path.join(root_dir, \"test_data.csv\")\n",
        "    return _load_csv(path)"
      ],
      "metadata": {
        "id": "9xN8-Ku1l7Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(base_path=\"../starter_code/data\"):\n",
        "    \"\"\" Load the data in PyTorch Tensor.\n",
        "\n",
        "    :return: (zero_train_matrix, train_data, valid_data, test_data)\n",
        "        WHERE:\n",
        "        zero_train_matrix: 2D sparse matrix where missing entries are\n",
        "        filled with 0.\n",
        "        train_data: 2D sparse matrix\n",
        "        valid_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "        test_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "    \"\"\"\n",
        "    train_matrix = load_train_sparse(base_path).toarray() #2D Sparse Matrix\n",
        "    train_data = load_train_csv(base_path)\n",
        "    valid_data = load_valid_csv(base_path) #{user id: [...], question id: [...], correct: [...]}\n",
        "    test_data = load_public_test_csv(base_path) #{user id: [...], q id: [...], correct: [...]}\n",
        "\n",
        "    zero_train_matrix = train_matrix.copy()\n",
        "\n",
        "    # Fill in the missing entries to 0.\n",
        "    zero_train_matrix[np.isnan(train_matrix)] = 0\n",
        "\n",
        "    # Change to Float Tensor for PyTorch.\n",
        "    zero_train_matrix = torch.FloatTensor(zero_train_matrix)\n",
        "    train_matrix = torch.FloatTensor(train_matrix)\n",
        "\n",
        "    return zero_train_matrix, train_matrix, train_data, valid_data, test_data"
      ],
      "metadata": {
        "id": "BRoSTZvNmJFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    torch.manual_seed(42)\n",
        "    def __init__(self, num_question, k=100):\n",
        "        \"\"\" Initialize a class AutoEncoder.\n",
        "\n",
        "        :param num_question: int\n",
        "        :param k: int\n",
        "        \"\"\"\n",
        "        super(AutoEncoder, self).__init__()\n",
        "\n",
        "        # Define linear functions.\n",
        "        self.encoder = nn.Linear(num_question, k) #self.g\n",
        "        self.decoder = nn.Linear(k, num_question) #self.h\n",
        "\n",
        "    def get_weight_norm(self):\n",
        "        \"\"\" Return ||W^1||^2 + ||W^2||^2.\n",
        "\n",
        "        :return: float\n",
        "        \"\"\"\n",
        "        g_w_norm = torch.norm(self.encoder.weight, 2) ** 2\n",
        "        h_w_norm = torch.norm(self.decoder.weight, 2) ** 2\n",
        "\n",
        "        return g_w_norm + h_w_norm\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Return a forward pass given inputs.\n",
        "\n",
        "        :param inputs: user vector.\n",
        "        :return: user vector.\n",
        "        \"\"\"\n",
        "        #####################################################################\n",
        "        # TODO:                                                             #\n",
        "        # Implement the function as described in the docstring.             #\n",
        "        # Use sigmoid activations for f and g.                              #\n",
        "        #####################################################################\n",
        "        x = torch.sigmoid(self.encoder(inputs))\n",
        "        out = torch.sigmoid(self.decoder(x))\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return out"
      ],
      "metadata": {
        "id": "B3Q7gfhYmT6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, train_data, valid_data):\n",
        "    \"\"\" Evaluate the valid_data on the current model.\n",
        "\n",
        "    :param model: Module\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param valid_data: A dictionary {user_id: list,\n",
        "    question_id: list, is_correct: list}\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    # Tell PyTorch you are evaluating the model.\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i, u in enumerate(valid_data[\"user_id\"]):\n",
        "        inputs = Variable(train_data[u]).unsqueeze(0)\n",
        "        output = model(inputs)\n",
        "\n",
        "        guess = output[0][valid_data[\"question_id\"][i]].item() >= 0.5\n",
        "        if guess == valid_data[\"is_correct\"][i]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / float(total)"
      ],
      "metadata": {
        "id": "_eBvYo-jlqVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, lr, lamb, train_matrix, zero_train_data, train_data, valid_data, num_epoch):\n",
        "    \"\"\" Train the neural network, where the objective also includes\n",
        "    a regularizer.\n",
        "\n",
        "    :param model: Module\n",
        "    :param lr: float\n",
        "    :param lamb: float\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param zero_train_data: 2D FloatTensor\n",
        "    :param train_data: Dict\n",
        "    :param valid_data: Dict\n",
        "    :param num_epoch: int\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Add a regularizer to the cost function.\n",
        "\n",
        "    # Tell PyTorch you are training the model.\n",
        "    model.train()\n",
        "\n",
        "    # Define optimizers and loss function.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    num_student = train_matrix.shape[0]\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    eps = []\n",
        "\n",
        "    for epoch in range(0, num_epoch):\n",
        "        train_loss = 0.\n",
        "        eps.append(epoch)\n",
        "\n",
        "        for user_id in range(num_student):\n",
        "            inputs = Variable(zero_train_data[user_id]).unsqueeze(0)  #answers to all questions by a student\n",
        "            target = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "\n",
        "            # Mask the target to only compute the gradient of valid entries.\n",
        "            nan_mask = np.isnan(train_matrix[user_id].unsqueeze(0).numpy())\n",
        "            target[0][nan_mask] = output[0][nan_mask]\n",
        "\n",
        "            loss = torch.sum((output - target) ** 2.)\n",
        "            loss.backward()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        train_acc = evaluate(model, zero_train_data, train_data)\n",
        "        valid_acc = evaluate(model, zero_train_data, valid_data)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(valid_acc)\n",
        "        print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "              \"Valid Acc: {}\".format(epoch, train_loss, valid_acc))\n",
        "\n",
        "      #plotting\n",
        "    plt.title(\"Training Loss vs. Epochs\")\n",
        "    plt.plot(eps, train_losses, label=\"Training Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Accuracy vs. Epochs\")\n",
        "    plt.plot(eps, train_accs, label=\"Training Curve\")\n",
        "    plt.plot(eps, val_accs, label=\"Validation Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################"
      ],
      "metadata": {
        "id": "KyNcCf5UmYm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  path = \"/content/drive/MyDrive/starter_code/data\"\n",
        "  zero_train_matrix, train_matrix, train_data, valid_data, test_data = load_data(path)\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Try out 5 different k and select the best k using the             #\n",
        "    # validation set.                                                   #\n",
        "    #####################################################################\n",
        "    # Set model hyperparameters.\n",
        "  k = 50\n",
        "  num_questions = zero_train_matrix.shape[1]\n",
        "  model = AutoEncoder(num_question = num_questions, k = k)\n",
        "\n",
        "  # Set optimization hyperparameters.\n",
        "  lr = 0.01\n",
        "  num_epoch = 40\n",
        "  lamb = 0\n",
        "\n",
        "  train(model, lr, lamb, train_matrix, zero_train_matrix, train_data,\n",
        "          valid_data, num_epoch)\n",
        "  #####################################################################\n",
        "  #                       END OF YOUR CODE                            #\n",
        "  #####################################################################"
      ],
      "metadata": {
        "id": "IRcc6tgsIyKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "5d3b0a64-986f-4e64-94d0-b636cd031cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 \tTraining Cost: 13504.900812\t Valid Acc: 0.6195314705052216\n",
            "Epoch: 1 \tTraining Cost: 12322.248103\t Valid Acc: 0.6344905447360993\n",
            "Epoch: 2 \tTraining Cost: 11671.075566\t Valid Acc: 0.6536833192209992\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-64e3e8f8f36f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   train(model, lr, lamb, train_matrix, zero_train_matrix,\n\u001b[0m\u001b[1;32m     21\u001b[0m           valid_data, num_epoch)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a9a73dd2503e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, lr, lamb, train_data, zero_train_data, valid_data, num_epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Mask the target to only compute the gradient of valid entries.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-236ed48aff00>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#                       END OF YOUR CODE                            #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularized NN"
      ],
      "metadata": {
        "id": "vG-IIIoD4G4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reg(model, lr, lamb, train_matrix, zero_train_data, train_data, valid_data, num_epoch):\n",
        "    \"\"\" Train the neural network, where the objective also includes\n",
        "    a regularizer.\n",
        "\n",
        "    :param model: Module\n",
        "    :param lr: float\n",
        "    :param lamb: float\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param zero_train_data: 2D FloatTensor\n",
        "    :param valid_data: Dict\n",
        "    :param num_epoch: int\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Add a regularizer to the cost function.\n",
        "    norm = model.get_weight_norm()\n",
        "    norm = norm.detach().numpy()\n",
        "    print(\"norm = \", norm)\n",
        "\n",
        "    # Tell PyTorch you are training the model.\n",
        "    model.train()\n",
        "\n",
        "    # Define optimizers and loss function.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    num_student = train_matrix.shape[0]\n",
        "\n",
        "    eps = []\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    for epoch in range(0, num_epoch):\n",
        "        train_loss = 0.\n",
        "\n",
        "        for user_id in range(num_student):\n",
        "            inputs = Variable(zero_train_data[user_id]).unsqueeze(0)  #answers to all questions by a student\n",
        "            target = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "\n",
        "            # Mask the target to only compute the gradient of valid entries.\n",
        "            nan_mask = np.isnan(train_matrix[user_id].unsqueeze(0).numpy())\n",
        "            target[0][nan_mask] = output[0][nan_mask]\n",
        "\n",
        "            loss = torch.sum(((output - target) ** 2.)) + lamb*norm/2 #check placement of norm term\n",
        "            loss.backward()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_acc = evaluate(model, zero_train_data, train_data)\n",
        "        valid_acc = evaluate(model, zero_train_data, valid_data)\n",
        "\n",
        "        eps.append(epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(valid_acc)\n",
        "        print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "              \"Valid Acc: {}\".format(epoch, train_loss, valid_acc))\n",
        "\n",
        "      #plotting\n",
        "    plt.title(\"Training Loss vs. Epochs\")\n",
        "    plt.plot(eps, train_losses, label=\"Training Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Accuracy vs. Epochs\")\n",
        "    plt.plot(eps, train_accs, label=\"Training Curve\")\n",
        "    plt.plot(eps, val_accs, label=\"Validation Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################"
      ],
      "metadata": {
        "id": "RxfkmVIB4KKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  path = \"/content/drive/MyDrive/starter_code/data\"\n",
        "  zero_train_matrix, train_matrix, train_data, valid_data, test_data = load_data(path)\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Try out 5 different k and select the best k using the             #\n",
        "    # validation set.                                                   #\n",
        "    #####################################################################\n",
        "    # Set model hyperparameters.\n",
        "  k = 50\n",
        "  num_questions = zero_train_matrix.shape[1]\n",
        "  model_reg = AutoEncoder(num_question = num_questions, k = k)\n",
        "\n",
        "  # Set optimization hyperparameters.\n",
        "  lr = 0.01\n",
        "  num_epoch = 40\n",
        "  lamb = 0.001\n",
        "\n",
        "  train_reg(model_reg, lr, lamb, train_matrix, zero_train_matrix, train_data, valid_data, num_epoch)\n",
        "  #####################################################################\n",
        "  #                       END OF YOUR CODE                            #\n",
        "  #####################################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJnaCIu14gCm",
        "outputId": "397a78dd-6e14-471f-ec83-d54269969b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm =  596.55023\n",
            "Epoch: 0 \tTraining Cost: 13674.634931\t Valid Acc: 0.622495060683037\n",
            "Epoch: 1 \tTraining Cost: 12463.829630\t Valid Acc: 0.6363251481795089\n",
            "Epoch: 2 \tTraining Cost: 11755.400912\t Valid Acc: 0.6515664690939882\n",
            "Epoch: 3 \tTraining Cost: 11162.529329\t Valid Acc: 0.6642675698560542\n",
            "Epoch: 4 \tTraining Cost: 10671.385089\t Valid Acc: 0.6699125035280835\n",
            "Epoch: 5 \tTraining Cost: 10273.309865\t Valid Acc: 0.6758396838837144\n",
            "Epoch: 6 \tTraining Cost: 9935.320303\t Valid Acc: 0.6804967541631386\n",
            "Epoch: 7 \tTraining Cost: 9642.127272\t Valid Acc: 0.6819079875811459\n",
            "Epoch: 8 \tTraining Cost: 9389.196759\t Valid Acc: 0.6809201241885408\n",
            "Epoch: 9 \tTraining Cost: 9156.072784\t Valid Acc: 0.6812023708721423\n",
            "Epoch: 10 \tTraining Cost: 8945.150385\t Valid Acc: 0.6799322607959356\n",
            "Epoch: 11 \tTraining Cost: 8771.104641\t Valid Acc: 0.6797911374541349\n",
            "Epoch: 12 \tTraining Cost: 8592.795979\t Valid Acc: 0.678662150719729\n",
            "Epoch: 13 \tTraining Cost: 8432.629589\t Valid Acc: 0.6775331639853232\n",
            "Epoch: 14 \tTraining Cost: 8285.458994\t Valid Acc: 0.6779565340107254\n",
            "Epoch: 15 \tTraining Cost: 8143.978165\t Valid Acc: 0.6785210273779283\n",
            "Epoch: 16 \tTraining Cost: 8018.601536\t Valid Acc: 0.6752751905165114\n",
            "Epoch: 17 \tTraining Cost: 7898.772879\t Valid Acc: 0.6734405870731018\n",
            "Epoch: 18 \tTraining Cost: 7779.305832\t Valid Acc: 0.6730172170476997\n",
            "Epoch: 19 \tTraining Cost: 7684.152042\t Valid Acc: 0.6727349703640982\n",
            "test accuracy =  0.5094552639006492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 4: Bagging -- Devansh"
      ],
      "metadata": {
        "id": "K2kVbZLEleEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import utils\n",
        "\n",
        "def load_data_for_bagging(base_path=\"../starter_code/data\"):\n",
        "    \"\"\" Load the data in PyTorch Tensor.\n",
        "\n",
        "    :return: (zero_train_matrix, train_data, valid_data, test_data)\n",
        "        WHERE:\n",
        "        zero_train_matrix: 2D sparse matrix where missing entries are\n",
        "        filled with 0.\n",
        "        train_data: 2D sparse matrix\n",
        "        valid_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "        test_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "    \"\"\"\n",
        "    train_matrix = load_train_sparse(base_path).toarray() #2D Sparse Matrix\n",
        "    train_data = load_train_csv(base_path)\n",
        "    valid_data = load_valid_csv(base_path) #{user id: [...], question id: [...], correct: [...]}\n",
        "    test_data = load_public_test_csv(base_path) #{user id: [...], q id: [...], correct: [...]}\n",
        "\n",
        "    zero_train_matrix = train_matrix.copy()\n",
        "\n",
        "    # Fill in the missing entries to 0.\n",
        "    zero_train_matrix[np.isnan(train_matrix)] = 0\n",
        "\n",
        "    # Change to Float Tensor for PyTorch.\n",
        "    zero_train_matrix = torch.FloatTensor(zero_train_matrix)\n",
        "    train_matrix1 = torch.FloatTensor(train_matrix)\n",
        "\n",
        "    return zero_train_matrix, train_matrix1, train_data, valid_data, test_data, train_matrix"
      ],
      "metadata": {
        "id": "QxW0cu_Slmh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_bagging(model, lr, lamb, train_data, zero_train_data, valid_data, num_epoch):\n",
        "    \"\"\" Train the neural network, where the objective also includes\n",
        "    a regularizer.\n",
        "\n",
        "    :param model: Module\n",
        "    :param lr: float\n",
        "    :param lamb: float\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param zero_train_data: 2D FloatTensor\n",
        "    :param valid_data: Dict\n",
        "    :param num_epoch: int\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Add a regularizer to the cost function.\n",
        "    norm = model.get_weight_norm()\n",
        "    norm = norm.detach().numpy()\n",
        "    print(\"norm = \", norm)\n",
        "\n",
        "    # Tell PyTorch you are training the model.\n",
        "    model.train()\n",
        "\n",
        "    # Define optimizers and loss function.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "    num_student = train_data.shape[0]\n",
        "\n",
        "    for epoch in range(0, num_epoch):\n",
        "        train_loss = 0.\n",
        "\n",
        "        for user_id in range(num_student):\n",
        "            inputs = Variable(zero_train_data[user_id]).unsqueeze(0)  #answers to all questions by a student\n",
        "            target = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "\n",
        "            # Mask the target to only compute the gradient of valid entries.\n",
        "            nan_mask = np.isnan(train_data[user_id].unsqueeze(0).numpy())\n",
        "            target[0][nan_mask] = output[0][nan_mask]\n",
        "\n",
        "            loss = torch.sum(((output - target) ** 2.)) + lamb*norm/2\n",
        "            loss.backward()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "        valid_acc = evaluate(model, zero_train_data, valid_data)\n",
        "        print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "              \"Valid Acc: {}\".format(epoch, train_loss, valid_acc))\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################"
      ],
      "metadata": {
        "id": "iJAX1o3hrhI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def evaluate_bagging(model1, model2, model3, train_data, valid_data):\n",
        "    \"\"\" Evaluate the valid_data on the current model.\n",
        "\n",
        "    :param model: Module\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param valid_data: A dictionary {user_id: list,\n",
        "    question_id: list, is_correct: list}\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    # Tell PyTorch you are evaluating the model.\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    model3.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i, u in enumerate(valid_data[\"user_id\"]):\n",
        "        inputs = Variable(train_data[u]).unsqueeze(0)\n",
        "        output1 = model1(inputs)\n",
        "        output2 = model2(inputs)\n",
        "        output3 = model3(inputs)\n",
        "\n",
        "        guess = (((output1[0][valid_data[\"question_id\"][i]].item() >= 0.5)+(output2[0][valid_data[\"question_id\"][i]].item() >= 0.5)+(output3[0][valid_data[\"question_id\"][i]].item() >= 0.5))/3) >= 0.5\n",
        "\n",
        "        if guess == valid_data[\"is_correct\"][i]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / float(total)"
      ],
      "metadata": {
        "id": "yYJLMjqctg6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create list of randomly sorted iterables (equal to # of students)\n",
        "import random\n",
        "\n",
        "def bootstrap(train_matrix):\n",
        "  '''Input: np train_matrix\n",
        "  Returns: tensor zero_train_matrix2, train_matrix2, zero_train_matrix3,train_matrix3\n",
        "  '''\n",
        "\n",
        "  train_matrix2 = train_matrix.copy()\n",
        "  train_matrix3 = train_matrix.copy()\n",
        "\n",
        "  rand_list1 = list(range(train_matrix.shape[0]))\n",
        "  random.shuffle(rand_list1)\n",
        "  rand_list2 = list(range(train_matrix.shape[0]))\n",
        "  random.shuffle(rand_list2)\n",
        "\n",
        "  for i in range(train_matrix.shape[0]):\n",
        "    j = rand_list1[i]\n",
        "    k = rand_list2[i]\n",
        "\n",
        "    train_matrix2[i] = train_matrix[j]\n",
        "    train_matrix3[i] = train_matrix[k]\n",
        "\n",
        "  zero_train_matrix2 = train_matrix2.copy()\n",
        "  zero_train_matrix3 = train_matrix3.copy()\n",
        "\n",
        "  #Fill in the missing entries to 0.\n",
        "  zero_train_matrix2[np.isnan(train_matrix2)] = 0\n",
        "  zero_train_matrix3[np.isnan(train_matrix3)] = 0\n",
        "\n",
        "  # Change to Float Tensor for PyTorch.\n",
        "  zero_train_matrix2 = torch.FloatTensor(zero_train_matrix2)\n",
        "  train_matrix2 = torch.FloatTensor(train_matrix2)\n",
        "\n",
        "  zero_train_matrix3 = torch.FloatTensor(zero_train_matrix3)\n",
        "  train_matrix3 = torch.FloatTensor(train_matrix3)\n",
        "\n",
        "  return zero_train_matrix2, train_matrix2, zero_train_matrix3, train_matrix3"
      ],
      "metadata": {
        "id": "vdS0EBFTI4Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  path = \"/content/drive/MyDrive/starter_code/data\"\n",
        "\n",
        "\n",
        "  zero_train_matrix1, train_matrix1, train_data, valid_data, test_data, np_train_matrix = load_data_for_bagging(path)\n",
        "  zero_train_matrix2, train_matrix2, zero_train_matrix3, train_matrix3 = bootstrap(np_train_matrix)\n",
        "\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Try out 5 different k and select the best k using the             #\n",
        "    # validation set.                                                   #\n",
        "    #####################################################################\n",
        "\n",
        "\n",
        "  #train the models on the three training datasets\n",
        "\n",
        "    # Set model hyperparameters.\n",
        "  k = 50\n",
        "  num_questions = zero_train_matrix1.shape[1]\n",
        "  modelb1 = AutoEncoder(num_question = num_questions, k = k)\n",
        "  modelb2 = AutoEncoder(num_question = num_questions, k = k)\n",
        "  modelb3 = AutoEncoder(num_question = num_questions, k = k)\n",
        "\n",
        "    # Set optimization hyperparameters.\n",
        "  lr1 = 0.01; lr2 = 0.02; lr3 = 0.001\n",
        "  num_epoch1 = 40; num_epoch2 = 30; num_epoch3 = 40\n",
        "  lamb1 = 0.001; lamb2 = 0.001; lamb3 = 0.001\n",
        "\n",
        "  train_bagging(modelb1, lr1, lamb1, train_matrix1, zero_train_matrix1, valid_data, num_epoch1)\n",
        "  train_bagging(modelb2, lr2, lamb2, train_matrix2, zero_train_matrix2, valid_data, num_epoch2)\n",
        "  train_bagging(modelb3, lr3, lamb3, train_matrix3, zero_train_matrix3, valid_data, num_epoch3)\n",
        "\n",
        "\n",
        "  #make ensembled validation predictions and evaluate accuracy (only one epoch -- because we have the fully trained model)\n",
        "  val_acc_b = evaluate_bagging(modelb1, modelb2, modelb3, zero_train_matrix1,valid_data) #choice of train matrix doesn't matter because eval function takes the ztm list of user ids and makes predictions on that specific list\n",
        "\n",
        "  #make ensembled validation predictions and evaluate accuracy (only one epoch -- because we have the fully trained model)\n",
        "  test_acc_b = evaluate_bagging(modelb1, modelb2, modelb3, zero_train_matrix1,test_data)\n",
        "\n",
        "  print(\"Validation Accuracy = \", val_acc_b, \"     \", \"Test Accuracy = \", test_acc_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DQLEzUsrhTR",
        "outputId": "21c55eea-b537-443d-be4e-8ee3872cbac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rand_list1 type =  <class 'list'>\n",
            "Epoch: 0 \tTraining Cost: 14308.366125\t Valid Acc: 0.5294947784363534\n",
            "Epoch: 1 \tTraining Cost: 13931.602462\t Valid Acc: 0.5534857465424782\n",
            "Epoch: 2 \tTraining Cost: 13590.435765\t Valid Acc: 0.5781823313576065\n",
            "Epoch: 3 \tTraining Cost: 13226.372755\t Valid Acc: 0.5924357888794807\n",
            "Epoch: 4 \tTraining Cost: 12889.105661\t Valid Acc: 0.602314422805532\n",
            "Epoch: 5 \tTraining Cost: 12615.122409\t Valid Acc: 0.6095117132373694\n",
            "Epoch: 6 \tTraining Cost: 12404.062513\t Valid Acc: 0.6148744002257973\n",
            "Epoch: 7 \tTraining Cost: 12240.494475\t Valid Acc: 0.6189669771380186\n",
            "Epoch: 8 \tTraining Cost: 12108.988413\t Valid Acc: 0.6241885407846458\n",
            "Epoch: 9 \tTraining Cost: 11998.558067\t Valid Acc: 0.6243296641264465\n",
            "Epoch: 10 \tTraining Cost: 11902.030702\t Valid Acc: 0.626728760937059\n",
            "Epoch: 11 \tTraining Cost: 11814.823382\t Valid Acc: 0.6306802145074796\n",
            "Epoch: 12 \tTraining Cost: 11733.940060\t Valid Acc: 0.6323736946090883\n",
            "Epoch: 13 \tTraining Cost: 11657.408529\t Valid Acc: 0.6347727914197008\n",
            "Epoch: 14 \tTraining Cost: 11583.998919\t Valid Acc: 0.6360429014959075\n",
            "Epoch: 15 \tTraining Cost: 11513.073344\t Valid Acc: 0.6370307648885125\n",
            "Epoch: 16 \tTraining Cost: 11444.182965\t Valid Acc: 0.6370307648885125\n",
            "Epoch: 17 \tTraining Cost: 11376.890946\t Valid Acc: 0.6394298616991251\n",
            "Epoch: 18 \tTraining Cost: 11310.872581\t Valid Acc: 0.6408410951171324\n",
            "Epoch: 19 \tTraining Cost: 11245.904214\t Valid Acc: 0.6422523285351397\n",
            "Epoch: 20 \tTraining Cost: 11181.841026\t Valid Acc: 0.6464860287891617\n",
            "Epoch: 21 \tTraining Cost: 11118.603705\t Valid Acc: 0.6483206322325713\n",
            "Epoch: 22 \tTraining Cost: 11056.156458\t Valid Acc: 0.6494496189669772\n",
            "Epoch: 23 \tTraining Cost: 10994.495857\t Valid Acc: 0.6510019757267852\n",
            "Epoch: 24 \tTraining Cost: 10933.653045\t Valid Acc: 0.6536833192209992\n",
            "Epoch: 25 \tTraining Cost: 10873.664845\t Valid Acc: 0.6572114027660175\n",
            "Epoch: 26 \tTraining Cost: 10814.554224\t Valid Acc: 0.6586226361840248\n",
            "Epoch: 27 \tTraining Cost: 10756.343172\t Valid Acc: 0.6608806096528366\n",
            "Epoch: 28 \tTraining Cost: 10699.056334\t Valid Acc: 0.6622918430708439\n",
            "Epoch: 29 \tTraining Cost: 10642.715731\t Valid Acc: 0.663279706463449\n",
            "Epoch: 30 \tTraining Cost: 10587.336820\t Valid Acc: 0.663844199830652\n",
            "Epoch: 31 \tTraining Cost: 10532.926191\t Valid Acc: 0.6639853231724527\n",
            "Epoch: 32 \tTraining Cost: 10479.479022\t Valid Acc: 0.6648320632232572\n",
            "Epoch: 33 \tTraining Cost: 10426.980180\t Valid Acc: 0.6655376799322608\n",
            "Epoch: 34 \tTraining Cost: 10375.420422\t Valid Acc: 0.6666666666666666\n",
            "Epoch: 35 \tTraining Cost: 10324.820918\t Valid Acc: 0.6676545300592718\n",
            "Epoch: 36 \tTraining Cost: 10275.222098\t Valid Acc: 0.6669489133502682\n",
            "Epoch: 37 \tTraining Cost: 10226.649032\t Valid Acc: 0.6683601467682755\n",
            "Epoch: 38 \tTraining Cost: 10179.097241\t Valid Acc: 0.6689246401354784\n",
            "Epoch: 39 \tTraining Cost: 10132.540600\t Valid Acc: 0.6679367767428732\n",
            "Epoch: 0 \tTraining Cost: 13830.225859\t Valid Acc: 0.5927180355630821\n",
            "Epoch: 1 \tTraining Cost: 12682.818613\t Valid Acc: 0.615580016934801\n",
            "Epoch: 2 \tTraining Cost: 12066.879498\t Valid Acc: 0.6195314705052216\n",
            "Epoch: 3 \tTraining Cost: 11648.537994\t Valid Acc: 0.6217894439740334\n",
            "Epoch: 4 \tTraining Cost: 11289.705662\t Valid Acc: 0.6236240474174428\n",
            "Epoch: 5 \tTraining Cost: 10965.049845\t Valid Acc: 0.6206604572396275\n",
            "Epoch: 6 \tTraining Cost: 10673.198593\t Valid Acc: 0.6175557437200113\n",
            "Epoch: 7 \tTraining Cost: 10415.359302\t Valid Acc: 0.6133220434659893\n",
            "Epoch: 8 \tTraining Cost: 10186.493679\t Valid Acc: 0.6095117132373694\n",
            "Epoch: 9 \tTraining Cost: 9982.037909\t Valid Acc: 0.6064069997177534\n",
            "Epoch: 10 \tTraining Cost: 9796.476857\t Valid Acc: 0.6033022861981372\n",
            "Epoch: 11 \tTraining Cost: 9627.174049\t Valid Acc: 0.602314422805532\n",
            "Epoch: 12 \tTraining Cost: 9470.968650\t Valid Acc: 0.6009031893875247\n",
            "Epoch: 13 \tTraining Cost: 9324.139625\t Valid Acc: 0.6004798193621225\n",
            "Epoch: 14 \tTraining Cost: 9185.900050\t Valid Acc: 0.5970928591589049\n",
            "Epoch: 0 \tTraining Cost: 14175.451456\t Valid Acc: 0.550381033022862\n",
            "Epoch: 1 \tTraining Cost: 13454.289405\t Valid Acc: 0.5829805249788315\n",
            "Epoch: 2 \tTraining Cost: 12846.170063\t Valid Acc: 0.6059836296923511\n",
            "Epoch: 3 \tTraining Cost: 12425.906088\t Valid Acc: 0.6130397967823878\n",
            "Epoch: 4 \tTraining Cost: 12141.649232\t Valid Acc: 0.6200959638724245\n",
            "Epoch: 5 \tTraining Cost: 11928.265416\t Valid Acc: 0.6219305673158341\n",
            "Epoch: 6 \tTraining Cost: 11749.938496\t Valid Acc: 0.6220716906576348\n",
            "Epoch: 7 \tTraining Cost: 11589.248395\t Valid Acc: 0.6206604572396275\n",
            "Epoch: 8 \tTraining Cost: 11437.762041\t Valid Acc: 0.6208015805814282\n",
            "Epoch: 9 \tTraining Cost: 11291.965181\t Valid Acc: 0.6175557437200113\n",
            "Epoch: 10 \tTraining Cost: 11151.046892\t Valid Acc: 0.6181202370872142\n",
            "Epoch: 11 \tTraining Cost: 11014.671509\t Valid Acc: 0.6158622636184025\n",
            "Epoch: 12 \tTraining Cost: 10883.318280\t Valid Acc: 0.61346316680779\n",
            "Epoch: 13 \tTraining Cost: 10757.515488\t Valid Acc: 0.6124753034151849\n",
            "Epoch: 14 \tTraining Cost: 10637.404550\t Valid Acc: 0.6114874400225797\n",
            "Epoch: 15 \tTraining Cost: 10522.901651\t Valid Acc: 0.6109229466553768\n",
            "Epoch: 16 \tTraining Cost: 10414.072008\t Valid Acc: 0.6107818233135761\n",
            "Epoch: 17 \tTraining Cost: 10310.553278\t Valid Acc: 0.6073948631103585\n",
            "Epoch: 18 \tTraining Cost: 10211.964344\t Valid Acc: 0.6072537397685577\n",
            "Epoch: 19 \tTraining Cost: 10117.933679\t Valid Acc: 0.6057013830087496\n",
            "Epoch: 20 \tTraining Cost: 10028.085403\t Valid Acc: 0.6024555461473328\n",
            "Epoch: 21 \tTraining Cost: 9942.062328\t Valid Acc: 0.5992097092859159\n",
            "Epoch: 22 \tTraining Cost: 9859.588039\t Valid Acc: 0.598645215918713\n",
            "Epoch: 23 \tTraining Cost: 9780.476686\t Valid Acc: 0.5992097092859159\n",
            "Epoch: 24 \tTraining Cost: 9704.410752\t Valid Acc: 0.5979395992097093\n",
            "Epoch: 25 \tTraining Cost: 9631.078961\t Valid Acc: 0.5987863392605137\n",
            "Epoch: 26 \tTraining Cost: 9560.306545\t Valid Acc: 0.5969517358171041\n",
            "Epoch: 27 \tTraining Cost: 9491.983393\t Valid Acc: 0.5961049957662997\n",
            "Epoch: 28 \tTraining Cost: 9425.898691\t Valid Acc: 0.5962461191081004\n",
            "Epoch: 29 \tTraining Cost: 9361.874907\t Valid Acc: 0.5955405023990968\n",
            "Epoch: 30 \tTraining Cost: 9299.757965\t Valid Acc: 0.5946937623482924\n",
            "Epoch: 31 \tTraining Cost: 9239.402365\t Valid Acc: 0.5939881456392887\n",
            "Epoch: 32 \tTraining Cost: 9180.685144\t Valid Acc: 0.5927180355630821\n",
            "Epoch: 33 \tTraining Cost: 9123.596621\t Valid Acc: 0.5920124188540785\n",
            "Epoch: 34 \tTraining Cost: 9068.048728\t Valid Acc: 0.5914479254868755\n",
            "Epoch: 35 \tTraining Cost: 9013.901370\t Valid Acc: 0.5911656788032741\n",
            "Epoch: 36 \tTraining Cost: 8961.067596\t Valid Acc: 0.5898955687270675\n",
            "Epoch: 37 \tTraining Cost: 8909.512353\t Valid Acc: 0.5911656788032741\n",
            "Epoch: 38 \tTraining Cost: 8859.242349\t Valid Acc: 0.5914479254868755\n",
            "Epoch: 39 \tTraining Cost: 8810.156003\t Valid Acc: 0.5901778154106689\n",
            "Validation Accuracy =  0.6871295512277731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#make ensembled validation predictions and evaluate accuracy (only one epoch -- because we have the fully trained model)\n",
        "if __name__ == \"__main__\":\n",
        "  test_acc = evaluate_bagging(model1, model2, model3, zero_train_matrix1,test_data)"
      ],
      "metadata": {
        "id": "tnpYf-NzBqYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Modification (Neural Networks) - Doh"
      ],
      "metadata": {
        "id": "sMpM61l-o0PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_data(base_path=\"../data\"):\n",
        "    \"\"\" Load the data in PyTorch Tensor.\n",
        "\n",
        "    :return: (zero_train_matrix, train_matrix train_data, valid_data, test_data)\n",
        "        WHERE:\n",
        "        zero_train_matrix: 2D sparse matrix where missing entries are\n",
        "        filled with 0.\n",
        "        train_matrix: Original training 2D sparse matrix\n",
        "        train_data: 2D sparse matrix\n",
        "        valid_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "        test_data: A dictionary {user_id: list,\n",
        "        user_id: list, is_correct: list}\n",
        "    \"\"\"\n",
        "    train_matrix = load_train_sparse(base_path).toarray()\n",
        "    train_data = load_train_csv(base_path)\n",
        "    valid_data = load_valid_csv(base_path)\n",
        "    test_data = load_public_test_csv(base_path)\n",
        "\n",
        "    zero_train_matrix = train_matrix.copy()\n",
        "\n",
        "    # Fill in the missing entries to 0.\n",
        "    zero_train_matrix[np.isnan(train_matrix)] = 0\n",
        "\n",
        "    # Change to Float Tensor for PyTorch.\n",
        "    zero_train_matrix = torch.FloatTensor(zero_train_matrix)\n",
        "    train_matrix = torch.FloatTensor(train_matrix)\n",
        "\n",
        "    return zero_train_matrix, train_matrix, train_data, valid_data, test_data\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    def __init__(self, num_question, k=50, j=75):\n",
        "        \"\"\" Initialize a class AutoEncoder.\n",
        "\n",
        "        :param num_question: int\n",
        "        :param k: int\n",
        "        \"\"\"\n",
        "        super(AutoEncoder, self).__init__()\n",
        "\n",
        "        # Define linear functions.\n",
        "        self.encoder = nn.Linear(num_question, j)\n",
        "        self.layer1 = nn.Linear(j, k)\n",
        "        self.layer2 = nn.Linear(k, j)\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "        self.decoder = nn.Linear(j, num_question)\n",
        "\n",
        "    def get_weight_norm(self):\n",
        "        \"\"\" Return ||W^1||^2 + ||W^2||^2.\n",
        "\n",
        "        :return: float\n",
        "        \"\"\"\n",
        "        g_w_norm = torch.norm(self.encoder.weight, 2) ** 2\n",
        "        h_w_norm = torch.norm(self.decoder.weight, 2) ** 2\n",
        "        return g_w_norm + h_w_norm\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\" Return a forward pass given inputs.\n",
        "\n",
        "        :param inputs: user vector.\n",
        "        :return: user vector.\n",
        "        \"\"\"\n",
        "        #####################################################################\n",
        "        # TODO:                                                             #\n",
        "        # Implement the function as described in the docstring.             #\n",
        "        # Use sigmoid activations for f and g.                              #\n",
        "        #####################################################################\n",
        "        a = torch.sigmoid(self.encoder(inputs))\n",
        "        b = F.relu(self.layer1(a))\n",
        "        c = F.relu(self.layer2(b))\n",
        "        d = F.relu(self.dropout(c))\n",
        "        out = torch.sigmoid(self.decoder(d))\n",
        "        #####################################################################\n",
        "        #                       END OF YOUR CODE                            #\n",
        "        #####################################################################\n",
        "        return out\n",
        "\n",
        "\n",
        "def train(model, lr, lamb, train_matrix, zero_train_data, train_data,\n",
        "          valid_data, num_epoch):\n",
        "    \"\"\" Train the neural network, where the objective also includes\n",
        "    a regularizer.\n",
        "\n",
        "    :param model: Module\n",
        "    :param lr: float\n",
        "    :param lamb: float\n",
        "    :param train_matrix: 2D sparse matrix\n",
        "    :param train_data: 2D sparse matrix\n",
        "    :param zero_train_data: 2D FloatTensor\n",
        "    :param valid_data: Dict\n",
        "    :param num_epoch: int\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    # TODO: Add a regularizer to the cost function.\n",
        "    norm = model.get_weight_norm()\n",
        "    norm = norm.detach().numpy()\n",
        "\n",
        "    # Tell PyTorch you are training the model.\n",
        "    model.train()\n",
        "\n",
        "    # Define optimizers and loss function.\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.1)\n",
        "    num_student = train_matrix.shape[0]\n",
        "\n",
        "    # Parameters for early stopping\n",
        "    previous_acc = 0\n",
        "    patience = 3\n",
        "    count = 0\n",
        "\n",
        "    eps = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(0, num_epoch):\n",
        "        train_loss = 0.\n",
        "\n",
        "        for user_id in range(num_student):\n",
        "            inputs = Variable(zero_train_data[user_id]).unsqueeze(0)\n",
        "            target = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inputs)\n",
        "\n",
        "            # Mask the target to only compute the gradient of valid entries.\n",
        "            nan_mask = np.isnan(train_matrix[user_id].unsqueeze(0).numpy())\n",
        "            target[0][nan_mask] = output[0][nan_mask]\n",
        "\n",
        "            loss = torch.sum((output - target) ** 2.) + lamb * norm / 2\n",
        "            loss.backward()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "        train_acc = evaluate(model, zero_train_data, train_data)\n",
        "        valid_acc = evaluate(model, zero_train_data, valid_data)\n",
        "        print(\"Epoch: {} \\tTraining Cost: {:.6f}\\t \"\n",
        "              \"Valid Acc: {}\".format(epoch, train_loss, valid_acc))\n",
        "\n",
        "        eps.append(epoch)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(valid_acc)\n",
        "\n",
        "        # Early stopping\n",
        "        if valid_acc <= previous_acc:\n",
        "            count += 1\n",
        "            if count >= patience:\n",
        "                break\n",
        "\n",
        "        # Modify patience to 3 consecutive non-increasing iterations\n",
        "        elif valid_acc > previous_acc:\n",
        "            count = 0\n",
        "\n",
        "        previous_acc = valid_acc\n",
        "\n",
        "    # plotting\n",
        "    plt.title(\"Training Loss vs. Epochs\")\n",
        "    plt.plot(eps, train_losses, label=\"Training Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Training Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Accuracy vs. Epochs\")\n",
        "    plt.plot(eps, train_accs, label=\"Training Curve\")\n",
        "    plt.plot(eps, val_accs, label=\"Validation Curve\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "\n",
        "\n",
        "def evaluate(model, train_data, valid_data):\n",
        "    \"\"\" Evaluate the valid_data on the current model.\n",
        "\n",
        "    :param model: Module\n",
        "    :param train_data: 2D FloatTensor\n",
        "    :param valid_data: A dictionary {user_id: list,\n",
        "    question_id: list, is_correct: list}\n",
        "    :return: float\n",
        "    \"\"\"\n",
        "    # Tell PyTorch you are evaluating the model.\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i, u in enumerate(valid_data[\"user_id\"]):\n",
        "        inputs = Variable(train_data[u]).unsqueeze(0)\n",
        "        output = model(inputs)\n",
        "\n",
        "        guess = output[0][valid_data[\"question_id\"][i]].item() >= 0.5\n",
        "        if guess == valid_data[\"is_correct\"][i]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / float(total)\n",
        "\n",
        "\n",
        "def main():\n",
        "    zero_train_matrix, train_matrix, train_data, valid_data, test_data = \\\n",
        "        load_data()\n",
        "\n",
        "    #####################################################################\n",
        "    # TODO:                                                             #\n",
        "    # Try out 5 different k and select the best k using the             #\n",
        "    # validation set.                                                   #\n",
        "    #####################################################################\n",
        "    # Set model hyperparameters.\n",
        "    k = 50\n",
        "    j = 75\n",
        "    num_questions = zero_train_matrix.shape[1]\n",
        "    model = AutoEncoder(num_question=num_questions, k=k, j=j)\n",
        "\n",
        "    # Set optimization hyperparameters.\n",
        "    lr = 0.01\n",
        "    num_epoch = 40\n",
        "    lamb = 0.001\n",
        "\n",
        "    print(\"Validation ========================================================\")\n",
        "    train(model, lr, lamb, train_matrix, zero_train_matrix, train_data,\n",
        "          valid_data, num_epoch)\n",
        "    print(\"Test ==============================================================\")\n",
        "    train(model, lr, lamb, train_matrix, zero_train_matrix, train_data,\n",
        "          test_data, num_epoch)\n",
        "    #####################################################################\n",
        "    #                       END OF YOUR CODE                            #\n",
        "    #####################################################################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ziEUm_QcpB0R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "a2dafaa6-e233-4070-e944-93029275766c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ee2f05853417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ee2f05853417>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mzero_train_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ee2f05853417>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m         user_id: list, is_correct: list}\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_valid_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8bfaa8567266>\u001b[0m in \u001b[0;36mload_train_sparse\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_sparse.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         raise Exception(\"The specified path {} \"\n\u001b[0m\u001b[1;32m     39\u001b[0m                         \"does not exist.\".format(os.path.abspath(path)))\n\u001b[1;32m     40\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_npz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The specified path /data/train_sparse.npz does not exist."
          ]
        }
      ]
    }
  ]
}